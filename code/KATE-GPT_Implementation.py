# -*- coding: utf-8 -*-
"""KATE_Testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Ik0nTHK1AbEJWxs9Vy4D92CYfgZGC1c
"""

import argparse
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn import datasets
from matplotlib import pyplot as plt
from tqdm import tqdm
import sys
from transformers import RobertaTokenizer, RobertaModel
from transformers import AutoTokenizer, AutoModel
from transformers import BartTokenizer, BartModel
import torch
import os
# from utils import chunks
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import pairwise
import pickle
import re
from sentence_transformers import SentenceTransformer

#########################################################################################################################################
print("Is CUDA available? ", torch.cuda.is_available())
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# parser = argparse.ArgumentParser()
# parser.add_argument('--train_fname', default='wmt16-en-de', type=str)
# parser.add_argument('--dev_fname', default='wmt16-en-de', type=str)
# parser.add_argument('--encoder_name', default='bart-large', type=str,
#                    help='roberta-base, roberta-large, bart-base, bart-large, \
#                    bert-base-nli-stsb-mean-tokens, bert-large-nli-stsb-mean-tokens, \
#                    roberta-base-nli-stsb-mean-tokens, roberta-large-nli-mean-tokens, roberta-large-nli-stsb-mean-tokens')
# parser.add_argument('--metric', default='euclidean', type=str, help='euclidean or cosine')
# parser.add_argument('--embed_type', default='CLS', type=str, help='CLS or mean')
# parser.add_argument('--num_neighbors', default=100, type=int, help='10, 50, or 100')
# parser.add_argument('--Q', default='sentence', help='sentence, source, table, text, q')
# parser.add_argument('--A', default='label', help='label, target, sentence, code, a')
# parser.add_argument('--reversed', action='store_true')
# args = parser.parse_args()

Q = 'sentence'
A = 'sentence1'

# print("The training dataset is {}".format(args.train_fname))
# print("The dev dataset is {}".format(args.dev_fname))
# print("The encoder to get {} {} embeddings is {}".format(args.embed_type, args.metric, args.encoder_name))

reversed = True
train_fname = "/content/drive/MyDrive/Subha_Project/testing_csv/ASSET_TUNE.csv"
dev_fname = "/content/drive/MyDrive/Subha_Project/testing_csv/ASSET_TEST.csv"

HF_cache_dir = "/Research/huggingface/transformers/cached_transformers/"
encoder_name = "roberta-base"
tok = RobertaTokenizer.from_pretrained(encoder_name, cache_dir=HF_cache_dir)
model = RobertaModel.from_pretrained(encoder_name, cache_dir=HF_cache_dir)
# elif args.encoder_name == "roberta-large-SST-2":
#     tok = RobertaTokenizer.from_pretrained("you_roberta-large_SST-2_folder")
#     model = RobertaModel.from_pretrained("you_roberta-base_SST-2_folder/checkpoint-6315")

num_neighbors = 1
embed_type = "CLS"
metric = "cosine" # "cosine" # "euclidean"

#########################################################################################################################################

# sync_from_GDrive = "rclone sync -P remote:PhD_Research/GPT-3/{} dataset/".format(train_fname)
# print(sync_from_GDrive)
# os.system(sync_from_GDrive)
# sync_from_GDrive = "rclone sync -P remote:PhD_Research/GPT-3/{} dataset/".format(dev_fname)
# print(sync_from_GDrive)
# os.system(sync_from_GDrive)

# re separator: (?<![\\t].)\\t
#train_df = pd.read_csv(train_fname, sep='(?<![\\t].)\\t', quotechar='"', engine='python', header='infer', keep_default_na=False)
train_df = pd.read_csv(train_fname, quotechar='"', engine='python', header='infer', keep_default_na=False)
train_corpus = train_df.loc[:, Q].to_list()
train_labels = train_df.loc[:, A].to_list()
print(train_corpus[277])
print(train_labels[277])

train_indices = list(range(len(train_corpus)))
# train_indices = list(range(2000))

train_corpus = [train_corpus[train_index] for train_index in train_indices]
train_labels = [train_labels[train_index] for train_index in train_indices]
for k in range(len(train_labels)):
    if train_labels[k] == 0:
        train_labels[k] = 2
    else:
        train_labels[k] = 3

# (?<!")\\t|(?<=")\\t(?=")
#dev_df = pd.read_csv(dev_fname, sep='(?<![\\t].)\\t|\\t(?!\\")', quotechar='"', engine='python', header='infer', keep_default_na=False)
dev_df = pd.read_csv(dev_fname, quotechar='"', engine='python', header='infer', keep_default_na=False)
dev_corpus = dev_df.loc[:, Q].to_list()
dev_labels = dev_df.loc[:, A].to_list()
print(dev_corpus)
print(dev_labels)

task_name = dev_fname
#########################################################################################################################################
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    return sum_embeddings / sum_mask

from itertools import zip_longest

def chunks(lst, n):
    """Yield successive n-sized chunks from lst."""
#     for i in range(0, len(lst), n):
#         yield lst[i:i + n]
    return [lst[i:i + n] for i in range(0, len(lst), n)]

def decode(tok, model, corpus):
    embeddings = []

    if encoder_name == 'roberta-base' or encoder_name == 'roberta-large':
        print("Using non Sentence Transformer models")
        corpus_chunks = chunks(corpus, 32)
        print(corpus_chunks)
        for corpus_tmp in tqdm(corpus_chunks):
            encoding = tok.batch_encode_plus(corpus_tmp, padding=True, truncation=True)
            sentence_batch, attn_mask = encoding["input_ids"], encoding["attention_mask"]
            sentence_batch, attn_mask = torch.LongTensor(sentence_batch).to(device), torch.LongTensor(attn_mask).to(device)

            with torch.no_grad():
                embedding_output_batch = model(sentence_batch, attn_mask)
                if embed_type == 'mean':
                    sentence_embeddings = mean_pooling( embedding_output_batch, attn_mask)
                elif embed_type == 'CLS':
                    sentence_embeddings = embedding_output_batch[0][:, 0, :]
            embeddings.append(sentence_embeddings.detach().cpu().numpy())

    #         embedding_output_batch = model(sentence_batch, attn_mask)
    #         embeddings.append(embedding_output_batch[0][:, 0, :].detach().cpu())
            del sentence_batch, attn_mask, embedding_output_batch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    else:
        print("Using Sentence Transformer models")
        for corpus_tmp in tqdm(corpus_chunks):
            sentence_embeddings = model.encode(corpus_tmp)
            embeddings.append(sentence_embeddings)

    return np.concatenate(embeddings, axis=0)

#########################################################################################################################################

labels = np.asarray(dev_labels + train_labels)
unique_labels = list(set(labels))
dev_indices = [[] for _ in unique_labels]
for i, label in enumerate(labels):
    for j, unique_label in enumerate(unique_labels):
        if label == unique_label:
            dev_indices[j].append(i)

n_dev = len(dev_labels)
n_train = len(train_indices)

corpus = dev_corpus + train_corpus

if encoder_name == "bm25":
    pass
else: # deep learning model
    model.to(device)
    X = decode(tok, model, corpus)
emb_train = X[n_dev:]
emb_dev = X[:n_dev]

if metric == "euclidean":
    nbrs = NearestNeighbors(n_neighbors=num_neighbors, algorithm='ball_tree', n_jobs=-1).fit(emb_train)
    distances, indices = nbrs.kneighbors(emb_dev)
elif metric == "cosine":
    dist_matrix = pairwise.cosine_similarity(X=emb_dev, Y=emb_train)
    if reversed:
        values, indices = torch.topk(-torch.from_numpy(dist_matrix), k=num_neighbors, dim=-1)
    else:
        values, indices = torch.topk(torch.from_numpy(dist_matrix), k=num_neighbors, dim=-1)
    indices = indices.numpy()

train_indices_np = np.asarray(train_indices)
kNN_dev_train = [train_indices_np[indices[i]].reshape(1, -1) for i in range(len(indices))]
kNN_dev_train = np.concatenate(kNN_dev_train, axis=0)
print(kNN_dev_train.shape)

if reversed:
    # PIK = "./inference/kNN_pretraining/" + task_name + "_{}_{}_{}_reversed.dat".format(encoder_name, metric, embed_type)
    PIK = "/content/drive/MyDrive/Subha_Project/new_dat_files/ASSET_1_RobertaBase.dat"
else:
    PIK = "./inference/kNN_pretraining/" + task_name + "_{}_{}_{}.dat".format(encoder_name, metric, embed_type)

data = dict()
data["kNN_dev_train"] = kNN_dev_train

with open(PIK, "wb") as f:
    pickle.dump(data, f)

# sync_to_GDrive = "rclone sync -P {} remote:PhD_Research/GPT-3/kNN_pretraining/".format(PIK)
# print(sync_to_GDrive)
# os.system(sync_to_GDrive)
# print("Finish kNN preprocessing!")

import os

categories = dict()

categories["text_simp"] = dict()
categories["text_simp"]["Qs"] = ["Sentence"]
categories["text_simp"]["A"] = ["Sentence1"]

categories["SST-2"] = dict()
categories["SST-2"]["Qs"] = ["Sentence"]
categories["SST-2"]["A"] = ["Label"]

categories["IMDB"] = dict()
categories["IMDB"]["Qs"] = ["Sentence"]
categories["IMDB"]["A"] = ["Label"]

categories["IMDB_UnitTest300"] = dict()
categories["IMDB_UnitTest300"]["Qs"] = ["Sentence"]
categories["IMDB_UnitTest300"]["A"] = ["Label"]

categories["CoLA"] = dict()
categories["CoLA"]["Qs"] = ["Sentence"]
categories["CoLA"]["A"] = ["Label"]

categories["QNLI"] = dict()
categories["QNLI"]["Qs"] = ["Sentence"]
categories["QNLI"]["A"] = ["Label"]

categories["QQP"] = dict()
categories["QQP"]["Qs"] = ["Question1", "Question2"]
categories["QQP"]["A"] = ["Is_duplicate"]

categories["RTE"] = dict()
categories["RTE"]["Qs"] = ["Sentence1", "Sentence2"]
categories["RTE"]["A"] = ["Label"]

categories["STS-B"] = dict()
categories["STS-B"]["Qs"] = ["Sentence1", "Sentence2"]
categories["STS-B"]["A"] = ["Score"]

categories["MNLI"] = dict()
categories["MNLI"]["Qs"] = ["Sentence1", "Sentence2"]
categories["MNLI"]["A"] = ["Label1"]

categories["WNLI"] = dict()
categories["WNLI"]["Qs"] = ["Sentence1", "Sentence2"]
categories["WNLI"]["A"] = ["Label"]

categories["MRPC"] = dict()
categories["MRPC"]["Qs"] = ["#1 String", "#2 String"]
categories["MRPC"]["A"] = ["Quality"]

# categories["trivia_qa_UnitTest_train_100000_dev_200"] = dict()
# categories["trivia_qa_UnitTest_train_100000_dev_200"]["Qs"] = ["Q"]
# categories["trivia_qa_UnitTest_train_100000_dev_200"]["A"] = ["A"]
#
# categories["trivia_qa_UnitTest_train_100000_dev_1000"] = dict()
# categories["trivia_qa_UnitTest_train_100000_dev_1000"]["Qs"] = ["Q"]
# categories["trivia_qa_UnitTest_train_100000_dev_1000"]["A"] = ["A"]
#
# categories["trivia_qa_train_78785_dev_full"] = dict()
# categories["trivia_qa_train_78785_dev_full"]["Qs"] = ["Q"]
# categories["trivia_qa_train_78785_dev_full"]["A"] = ["A"]
#
# categories["web_qs_train_3778_dev_full"] = dict()
# categories["web_qs_train_3778_dev_full"]["Qs"] = ["Q"]
# categories["web_qs_train_3778_dev_full"]["A"] = ["A"]
#
# categories["web_qs_train_3417_dev_full"] = dict()
# categories["web_qs_train_3417_dev_full"]["Qs"] = ["Q"]
# categories["web_qs_train_3417_dev_full"]["A"] = ["A"]
#
# categories["natural_qs_train_79168_dev_full"] = dict()
# categories["natural_qs_train_79168_dev_full"]["Qs"] = ["Q"]
# categories["natural_qs_train_79168_dev_full"]["A"] = ["A"]

categories["ToTTo"] = dict()
categories["ToTTo"]["Qs"] = ["Table"]
categories["ToTTo"]["A"] = ["Sentence"]

categories["MT"] = dict()
categories["MT"]["Qs"] = ["Source"]
categories["MT"]["A"] = ["Target"]

categories["sentiment"] = dict()
categories["sentiment"]["Qs"] = ["Sentence"]
categories["sentiment"]["A"] = ["Label"]

categories["text2code"] = dict()
categories["text2code"]["Qs"] = ["Text"]
categories["text2code"]["A"] = ["Code"]

categories["QA"] = dict()
categories["QA"]["Qs"] = ["Q"]
categories["QA"]["A"] = ["A"]

templates = dict()

templates["text_simp"] = dict()
templates["text_simp"]["Qs"] = ["Complex Sentence:{}\n"]
templates["text_simp"]["A"] = ["Simple Sentence:{}\n"]

templates["MT"] = dict()
templates["MT"]["Qs"] = ['{} ==']
templates["MT"]["A"] = [' {}\n\n', '']

templates["sentiment"] = dict()
templates["sentiment"]["Qs"] = ['Sentence:{} ']
templates["sentiment"]["A"] = ['Label:{}\n\n', 'Label:']

templates["ToTTo"] = dict()
templates["ToTTo"]["Qs"] = ['Table:{} ']
templates["ToTTo"]["A"] = ['Sentence:{}\n\n', 'Sentence:']

templates["text2code"] = dict()
templates["text2code"]["Qs"] = ["Text:{} "]
templates["text2code"]["A"] = ["Code:{}\n\n", 'Code:']

templates["QA"] = dict()
templates["QA"]["Qs"] = ["Q: {}\n"]
templates["QA"]["A"] = ["A: {}\n\n", "A:"]

# -*- coding: utf-8 -*-
# https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences
import re
alphabets= "([A-Za-z])"
prefixes = "(Mr|St|Mrs|Ms|Dr|mr|st|mrs|ms|dr)[.]"
suffixes = "(Inc|Ltd|Jr|Sr|Co)"
starters = "(Mr|Mrs|Ms|Dr|He\s|She\s|It\s|They\s|Their\s|Our\s|We\s|But\s|However\s|That\s|This\s|Wherever)"
acronyms = "([A-Z][.][A-Z][.](?:[A-Z][.])?)"
websites = "[.](com|net|org|io|gov)"

digits = "([0-9])"

def split_into_sentences(text):
    text = " " + text + "  "
    text = text.replace("\n"," ")
    text = re.sub(prefixes,"\\1<prd>",text)
    text = re.sub(websites,"<prd>\\1",text)

    text = re.sub(digits + "[.]" + digits,"\\1<prd>\\2",text)

    if "Ph.D" in text: text = text.replace("Ph.D.","Ph<prd>D<prd>")
    text = re.sub("\s" + alphabets + "[.] "," \\1<prd> ",text)
    text = re.sub(acronyms+" "+starters,"\\1<stop> \\2",text)
    text = re.sub(alphabets + "[.]" + alphabets + "[.]" + alphabets + "[.]","\\1<prd>\\2<prd>\\3<prd>",text)
    text = re.sub(alphabets + "[.]" + alphabets + "[.]","\\1<prd>\\2<prd>",text)
    text = re.sub(" "+suffixes+"[.] "+starters," \\1<stop> \\2",text)
    text = re.sub(" "+suffixes+"[.]"," \\1<prd>",text)
    text = re.sub(" " + alphabets + "[.]"," \\1<prd>",text)
    if "”" in text: text = text.replace(".”","”.")
    if "\"" in text: text = text.replace(".\"","\".")
    if "!" in text: text = text.replace("!\"","\"!")
    if "?" in text: text = text.replace("?\"","\"?")
    text = text.replace(".",".<stop>")
    text = text.replace("?","?<stop>")
    text = text.replace("!","!<stop>")
    text = text.replace("<prd>",".")
    sentences = text.split("<stop>")
    sentences = sentences[:-1]
    sentences = [s.strip() for s in sentences]
    return sentences

# https://stackoverflow.com/questions/1518522/find-the-most-common-element-in-a-list
from collections import Counter
def most_common(lst):
    data = Counter(lst)
    return max(lst, key=data.get)

# https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks
def chunks(lst, n):
    """Yield successive n-sized chunks from lst."""
#     for i in range(0, len(lst), n):
#         yield lst[i:i + n]
    return [lst[i:i + n] for i in range(0, len(lst), n)]

def truncSent(sentence, words_count=175):
    new_sent = sentence.split(' ')
    if len(new_sent) > words_count:
        new_sent = ' '.join(new_sent[:words_count])
        return new_sent
    return sentence

def constructPrompt(df, labels, indices, templates, Q_list, A_list, A_flag=False, truncate=False):
  # tmp_example = constructPrompt(df=train_df, labels=train_labels, indices=kNN_dev_train[single_list[l]][
  #                                     :knn_num], templates=templates, Q_list=Q_list, A_list=A_list, A_flag=True, truncate=truncate)
    tmp_example = ''
    Q_templates = templates['Qs']
    A_templates = templates['A']
    # print("A_list" + A_list)
    for tmp_index in indices:
        for i, Q in enumerate(Q_list):
            if truncate:
                #print(Q.lower())
                #print(df.loc[tmp_index, Q.lower()])
                series2 = df.loc[tmp_index, Q.lower()]
                # print("SERIES 2")
                # print(series2)
                tmp_example += Q_templates[i].format(truncSent(series2))
            else:
                tmp_example += Q_templates[i].format(df.loc[tmp_index, Q.lower()])
        if A_flag:
            for A in A_list:
                if truncate:
                    series3 = df.loc[tmp_index, A.lower()]
                    tmp_example += A_templates[0].format(truncSent(series3))
                else:
                    tmp_example += A_templates[0].format(df.loc[tmp_index, A.lower()])
        # else:
            # for A in A_list:
                # tmp_example += A_templates[1]
    return tmp_example


#    tmp_example = ""
#    for tmp_index in indices:
#        for Q in Q_list:
#            if truncate:
#                tmp_example += Q + ': ' + truncSent(df.loc[tmp_index, Q.lower()]) + '\n'
#            else:
#                tmp_example += Q + ': ' + df.loc[tmp_index, Q.lower()] + '\n'
#        if A_flag:
#            for A in A_list:
#                if truncate:
#                    tmp_example += A + ': ' + truncSent(df.loc[tmp_index, A.lower()]) + '\n'
#                else:
#                    tmp_example += A + ': ' + df.loc[tmp_index, A.lower()] + '\n'
##                 tmp_example += A + ':' + labels[tmp_index] + '\n'
#            tmp_example += '\n'
#        else:
#            for A in A_list:
#                tmp_example += A + ':'
#    return tmp_example

def cleanLabel(labels, digits):
    for i, label in enumerate(labels):
        if label == 1:
            if digits:
                labels[i] = '1'
            else:
                labels[i] = 'positive'
        elif label == 0:
            if digits:
                labels[i] = '0'
            else:
                labels[i] = 'negative'

        if label == "entailment":
            labels[i] = 'entailment'
        elif label == "not_entailment":
            labels[i] = 'non-entailment'
    return

import os
import sys
import pandas as pd
import numpy as np
import random
from tqdm import tqdm
import pickle
import argparse
import sacrebleu
import itertools
# from utils import categories, templates, chunks, constructPrompt, cleanLabel, most_common
import openai
openai.api_key = "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"

categories = dict()
templates = dict()
categories["text_simp"] = dict()
categories["text_simp"]["Qs"] = ["Sentence"]
categories["text_simp"]["A"] = ["Sentence1"]

templates["text_simp"] = dict()
templates["text_simp"]["Qs"] = ["Complex Sentence: {}\nSimple Sentence: "]
templates["text_simp"]["A"] = ["Simple Sentence: {}\n--\n"]

# parser = argparse.ArgumentParser()
# parser.add_argument('--task_type', default='classification', type=str,
#                     help="classification or generation")
# parser.add_argument('--task_name', default='SST-2', type=str)
# parser.add_argument('--train_name', default='', type=str)
# parser.add_argument('--category_name', default='', type=str)
# parser.add_argument('--epochs', default=10, type=int)
# parser.add_argument('--bz_train', default=1, type=int)
# parser.add_argument('--knn_num', default=1, type=int)
# parser.add_argument('--bz_dev', default=1, type=int)
# parser.add_argument('--max_tokens', default=10, type=int)
# parser.add_argument('--digits', action='store_true')
# parser.add_argument('--truncate', action='store_true')
# parser.add_argument('--kNN_dev_train', default='', type=str)
# parser.add_argument('--evaluate_train', action='store_true')
# parser.add_argument('--evaluate_train_indices',
#                     nargs='+', default=[-1], type=int)
# parser.add_argument('--evaluate_dev', action='store_true')
# parser.add_argument('--PIK_name', default="tmp", type=str)
# args = parser.parse_args()
import time

##############################################################
# GPT-3 completion class


class glueComplete:
    def __init__(self):
        return

    def __call__(self, example="", max_tokens=50):
        prompt = example
        response = openai.Completion.create(
    model="text-davinci-003",
    prompt=example,
    temperature=0.7,
    max_tokens=256,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0
    )
        resp = []
        for c in response.choices:
            text = c.text
            resp.append(text.strip())
        with open(f"/content/drive/MyDrive/Subha_Project/new_generated/ASSET_1_RobertaBase.txt", 'a') as f:
            stringthing = response['choices'][0]['text']
            stringthing2 = stringthing.replace("\n\n", "")
            stringthing3 = stringthing2.strip()
            f.write(stringthing3)
            f.write("\n")
        return resp


##############################################################
task = glueComplete()

##############################################################
# experiment parameters

task_type = "generation"
task_name = "text_simp"
train_name = "text_simp"
category_name = "text_simp"
epochs = 1
bz_train = 1
knn_num = 1
bz_dev = 1
max_tokens = 50
digits = True
truncate = True
# args.kNN_dev_train not needed since code is modified
evaluate_train = False
evaluate_train_indices = -1
evaluate_dev = True
PIK_name = "tmp"
##############################################################
# read files
train_fname = "/content/drive/MyDrive/Subha_Project/testing_csv/ASSET_TUNE.csv"
dev_fname = "/content/drive/MyDrive/Subha_Project/testing_csv/ASSET_TEST.csv"
train_df = pd.read_csv(train_fname, quotechar='"', engine='python', header='infer', keep_default_na=False)
dev_df = pd.read_csv(dev_fname, quotechar='"', engine='python', header='infer', keep_default_na=False)
print(len(dev_df))
##############################################################

train_indices = range(len(train_df))
# test_indices is called dev_indices
train_labels = train_df.loc[:,
                            categories[category_name]["A"][0].lower()].to_list()
dev_labels = dev_df.loc[:, categories[category_name]["A"][0].lower()].to_list()

if task_type == "classification":
    dev_unique_labels = list(set(dev_labels))
    dev_unique_labels.sort()
    dev_indices = [[] for _ in dev_unique_labels]
    for j, x in enumerate(dev_labels):
        for i, y in enumerate(dev_unique_labels):
            if x == y:
                dev_indices[i].append(j)
elif task_type == "generation":
    dev_unique_labels = [0]
    dev_indices = [list(range(len(dev_df)))]

track_train = []  # EPOCHS x N, indices for train set samples
# D x M, indices for dev set samples
track_dev = [[] for _ in dev_unique_labels]
# D x EPOCHS x M, prediction for dev set samples
pred_dev = [[] for _ in dev_unique_labels]
# EPOCHS x D, accuracy for dev set samples
accuracy_dev = [[] for _ in dev_unique_labels]

# evaluate on the entire dev set or random samples for each epoch
for i in range(len(dev_unique_labels)):
    track_dev[i].append(dev_indices[i])

# prompt construction using kNN or random samples or fixed samples
evaluate_train_indices = []
PIK_kNN = "/content/drive/MyDrive/Subha_Project/new_dat_files/ASSET_1_RobertaBase.dat"
with open(PIK_kNN, "rb") as f:
    kNN_data = pickle.load(f)
    kNN_dev_train = kNN_data["kNN_dev_train"]

print(kNN_dev_train)

print("Evaluate on dev set using KNN on train set")

# if args.evaluate_train:
#     evaluate_train_indices = args.evaluate_train_indices
#     print("Evaluate on training indices:", evaluate_train_indices)
##############################################################
# actual GPT-3 few-shot learning

Q_list = categories[category_name]["Qs"]
A_list = categories[category_name]["A"]
templates = templates[category_name]

for _ in tqdm(range(epochs)):

    # we don't want it to evaluate on random samples from the training set, so we change this to the dev set

    if evaluate_train:
        tmp_train_indices = evaluate_train_indices
    else:
        tmp_train_indices = dev_indices
    # track_train.append(tmp_train_indices) # store the training indices during each iteration

    # tmp_example = constructPrompt(df=dev_df, labels=dev_labels, indices=tmp_train_indices,
    #                               templates=templates, Q_list=Q_list, A_list=A_list, A_flag=True, truncate=truncate)

    # we deleted the for i in range(len(dev_unique_labels)): because there are no dev_unique_labels because it is a generation task

    tmp_dev_indices = dev_indices
    counts = 0
    pred_dev[i].append([])

    # We don't really need to chunk anything so I deleted the line for dev_indices in tqdm(chunks(tmp_dev_indices, n=20)):
    single_list = list(itertools.chain(*dev_indices))
    prompt_example = []
    k = 0
    # Change this to len(single_list)
    for p in range(3,359):
        # Deleted if args.kNN_dev_train: since we don't need it ??
        #print(train_df.loc[0])

        # Changed this to single_list
        #print("KNN DEV TRAIN")
        #for k in range(0, knn_num-1):
      #print(kNN_dev_train[0,0])

      #print(kNN_dev_train[single_list[p]][:6])
      tmp_example = constructPrompt(df=train_df, labels=train_labels, indices=kNN_dev_train[single_list[p]][
                                    :knn_num], templates=templates, Q_list=Q_list, A_list=A_list, A_flag=True, truncate=truncate)

      #print(tmp_example)
      #prompt_example.append(tmp_example)
      #print(prompt_example)


      dev_prompt = constructPrompt(df=dev_df, labels=dev_labels, indices=[
                                      single_list[p]], templates=templates, Q_list=Q_list, A_list=A_list, A_flag=False, truncate=truncate)
      example_to_model = ""
      example_to_model += tmp_example
      example_to_model += dev_prompt
      prompt_example.append(example_to_model)

    #print("PROMPT EXAMPLE")
      #print(p)
      #print(prompt_example)
      #print(prompt_example[k])

      tmp_pred = task(example=prompt_example[k], max_tokens=256)
      pred_dev[i][-1] += tmp_pred
      k+=1

##############################################################
# summarize and save into file
# print(accuracy_dev)
# PIK = "result/" + task_name + "/" + args.PIK_name + ".dat"

data = dict()
data["task"] = task_name
data["bz_dev"] = bz_dev
data["bz_train"] = bz_train
data["epochs"] = epochs
data["max_tokens"] = max_tokens
data["evaluate_train"] = evaluate_train
data["evaluate_dev"] = evaluate_dev
data["track_train"] = track_train
data["track_dev"] = track_dev
data["pred_dev"] = pred_dev
data["accuracy_dev"] = accuracy_dev

# with open(PIK, "wb") as f:
#     pickle.dump(data, f)